import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

# ==========================================
# è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (å¥½ã¿ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„)
# ==========================================
model_id = "LiquidAI/LFM2-2.6B-Exp"

# ç”Ÿæˆæ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
GEN_CONFIG = {
    "max_new_tokens": 1024,  # å¿œç­”ã®æœ€å¤§æ–‡å­—æ•°ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰ã€‚é•·æ–‡ã«ã—ãŸã„å ´åˆã¯å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚
    "temperature": 0.7,      # å‰µé€ æ€§ã€‚é«˜ã„ã»ã©ç‹¬å‰µçš„ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰ã€ä½ã„ã»ã©å …å®Ÿãªå›ç­”ã«ãªã‚Šã¾ã™ã€‚
    "do_sample": True,       # Trueã§ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–ã€‚
    "top_p": 0.9,            # ç´¯ç©ç¢ºç‡ãŒã“ã‚Œã«é”ã™ã‚‹ã¾ã§ã®ä¸Šä½ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’è€ƒæ…®ã€‚
}

# å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
# Apple Silicon Macã§ã¯ "mps" ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§GPUåŠ é€Ÿï¼ˆMetalï¼‰ãŒåŠ¹ãã¾ã™ã€‚
DEVICE = "mps" if torch.backends.mps.is_available() else "cpu"

def main():
    print(f"Loading {model_id} ...")
    print("-" * 50)
    print("ã€ãƒ’ãƒ³ãƒˆã€‘")
    print("ãƒ»ãƒ¢ãƒ‡ãƒ«ã¯ç´„5GBã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ã§ã™ï¼ˆåˆå›ã®ã¿ï¼‰ã€‚")
    print("ãƒ»ä¸€åº¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒçµ‚ã‚ã‚Œã°ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ï¼ˆãƒãƒƒãƒˆãªã—ï¼‰ã§ã‚‚å‹•ä½œã—ã¾ã™ã€‚")
    print("-" * 50)

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆæ–‡å­—ã¨æ•°å­—ã‚’å¤‰æ›ã™ã‚‹è¾æ›¸ï¼‰ã®èª­ã¿è¾¼ã¿
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã®èª­ã¿è¾¼ã¿
    # torch_dtype=torch.float16: M4 Macç­‰ã§æœ€ã‚‚åŠ¹ç‡ã‚ˆãå‹•ãæµ®å‹•å°æ•°ç‚¹ç²¾åº¦ã‚’æŒ‡å®š
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
    ).to(DEVICE)

    print(f"\nâœ… æº–å‚™å®Œäº†ï¼ (ãƒ‡ãƒã‚¤ã‚¹: {DEVICE})")
    print("çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›ã€ã¾ãŸã¯ Ctrl + C ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿æŒã™ã‚‹ãƒªã‚¹ãƒˆ
    messages = []

    while True:
        try:
            user_input = input("\nğŸ‘¤ ã‚ãªãŸ: ")
            if not user_input.strip():
                continue
            
            if user_input.lower() in ["exit", "quit", "çµ‚äº†"]:
                break

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å±¥æ­´ã«è¿½åŠ 
            messages.append({"role": "user", "content": user_input})
            
            # --- 1. ãƒ‡ãƒã‚¤ã‚¹å‘ã‘ã«å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ ---
            # apply_chat_template: ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã—ã‚„ã™ã„å¯¾è©±å½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›
            inputs = tokenizer.apply_chat_template(
                messages, 
                add_generation_prompt=True, 
                return_tensors="pt",
                return_dict=True
            ).to(model.device)

            # --- 2. AIã®å¿œç­”ç”Ÿæˆ ---
            # **GEN_CONFIG ã‚’å±•é–‹ã—ã¦å¼•æ•°ã«æ¸¡ã—ã¦ã„ã¾ã™
            outputs = model.generate(
                **inputs, 
                **GEN_CONFIG,
                pad_token_id=tokenizer.eos_token_id
            )

            # --- 3. å¿œç­”ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆæ•°å€¤ã‹ã‚‰æ–‡å­—ã¸å¤‰æ›ï¼‰ ---
            # å…¥åŠ›éƒ¨åˆ†ã®é•·ã•ï¼ˆinput_lengthï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸéƒ¨åˆ†ã®ã¿ã‚’å–ã‚Šå‡ºã™
            input_length = inputs["input_ids"].shape[1]
            response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
            
            print(f"\nğŸ’§ Liquid AI: {response}")

            # AIã®å¿œç­”ã‚‚å±¥æ­´ã«ä¿å­˜ï¼ˆã“ã‚Œã«ã‚ˆã£ã¦éå»ã®æ–‡è„ˆã‚’ç†è§£ã§ãã¾ã™ï¼‰
            messages.append({"role": "assistant", "content": response})
            
        except KeyboardInterrupt:
            print("\n\nãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        except Exception as e:
            print(f"\nâš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            continue

if __name__ == "__main__":
    main()